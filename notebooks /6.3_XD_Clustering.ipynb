{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow imports from parent directory \n",
    "import os, sys\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    os.chdir(\"..\")\n",
    "    sys.path.append(os.path.abspath(\".\")) \n",
    "    \n",
    "from astropy.io import fits\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.coordinates import SkyCoord\n",
    "import astropy.units as u\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import pickle\n",
    "from extreme_deconvolution import extreme_deconvolution\n",
    "from scipy.linalg import det \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "import json\n",
    "\n",
    "# File path\n",
    "filtered_corrected_data = 'data/Allsky_Gaia_42481846_extinction_corrected_filtered.fits'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extreme Deconvolution Pipeline\n",
    "- This pipeline was originally built to apply extreme deconvolution to the whole dataset\n",
    "- This yielded terrible results as it wasnt able seperate noise from tiny scale structures\n",
    "- It could now be used to take the results from my custom algorithm and seperate noise from cluster, ie create a inner spherical distiribution from the 4D bin\n",
    "- I did not have time to run this and process results so the pipline is provided here as an extension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load matched clusters from the JSON file\n",
    "with open(\"data/matched_clusters_LowerPM.json\", \"r\") as f:\n",
    "    matched_clusters_dict_lower = json.load(f)\n",
    "\n",
    "# Convert merged dictionary into a list for plotting\n",
    "cluster_ranges = [\n",
    "    (cluster_name, data[\"l_min\"], data[\"l_max\"], data[\"b_min\"], data[\"b_max\"], data[\"pm_ra_min\"], data[\"pm_ra_max\"], data[\"pm_dec_min\"], data[\"pm_dec_max\"])\n",
    "    for cluster_name, data in matched_clusters_dict_lower.items()\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaination:\n",
    "- Per hypercube this tried to fit two gaussians, one for the background and one for the cluster\n",
    "- It selects the gaussian with the small convariance matrix diagonals as it that this will be the guassian fitting the cluster \n",
    "- As the background gaussian will be a broader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "final_results = []\n",
    "\n",
    "# Iterate over all identified clusters\n",
    "for cluster in tqdm(cluster_ranges, desc=\"Processing Clusters\", dynamic_ncols=True):\n",
    "    cluster_id, l_min, l_max, b_min, b_max, pm_ra_min, pm_ra_max, pm_dec_min, pm_dec_max = cluster\n",
    "\n",
    "    # Extend region in RA/Dec for better selection\n",
    "    deg_range = (max(l_max - l_min, b_max - b_min) * 1.5) / 2\n",
    "    l_med, b_med = (l_min + l_max) / 2, (b_min + b_max) / 2\n",
    "\n",
    "    # Convert central L, B to RA, DEC\n",
    "    coord_med = SkyCoord(l=l_med * u.degree, b=b_med * u.degree, frame='galactic')\n",
    "    ra_med, dec_med = coord_med.icrs.ra.degree, coord_med.icrs.dec.degree\n",
    "\n",
    "    # Define extended search region\n",
    "    ra_min_ext, ra_max_ext = ra_med - deg_range, ra_med + deg_range\n",
    "    dec_min_ext, dec_max_ext = dec_med - deg_range, dec_med + deg_range\n",
    "    pm_ra_min_ext, pm_ra_max_ext = pm_ra_min - 0.25 * (pm_ra_max - pm_ra_min), pm_ra_max + 0.25 * (pm_ra_max - pm_ra_min)\n",
    "    pm_dec_min_ext, pm_dec_max_ext = pm_dec_min - 0.25 * (pm_dec_max - pm_dec_min), pm_dec_max + 0.25 * (pm_dec_max - pm_dec_min)\n",
    "\n",
    "    # Extract the data from the FITS file\n",
    "    with fits.open(filtered_corrected_data, memmap=True) as hdul:\n",
    "        data = hdul[1].data \n",
    "        ra, dec, pm_ra, pm_dec = data['ra'], data['dec'], data['pmra'], data['pmdec']\n",
    "        ra_error, dec_error = data['ra_error'], data['dec_error']\n",
    "        \n",
    "        pm_ra_error, pm_dec_error = data['pmra_error'], data['pmdec_error']\n",
    "\n",
    "        # Filter the data within the extended region\n",
    "        mask = (\n",
    "            (ra >= ra_min_ext) & (ra <= ra_max_ext) &\n",
    "            (dec >= dec_min_ext) & (dec <= dec_max_ext) &\n",
    "            (pm_ra >= pm_ra_min_ext) & (pm_ra <= pm_ra_max_ext) &\n",
    "            (pm_dec >= pm_dec_min_ext) & (pm_dec <= pm_dec_max_ext)\n",
    "        )\n",
    "\n",
    "        # Extract relevant data\n",
    "        cluster_ra_data, cluster_dec_data = ra[mask], dec[mask]\n",
    "        cluster_pm_ra_data, cluster_pm_dec_data = pm_ra[mask], pm_dec[mask]\n",
    "        cluster_ra_error, cluster_dec_error = ra_error[mask], dec_error[mask]\n",
    "        cluster_pm_ra_error, cluster_pm_dec_error = pm_ra_error[mask], pm_dec_error[mask]\n",
    "\n",
    "        # If no data, skip\n",
    "        if len(cluster_ra_data) < 10:\n",
    "            continue\n",
    "\n",
    "        # Standardize data\n",
    "        cluster_data_set = np.stack([cluster_ra_data, cluster_dec_data, cluster_pm_ra_data, cluster_pm_dec_data], axis=1)\n",
    "        cluster_data_set_error = np.stack([cluster_ra_error, cluster_dec_error, cluster_pm_ra_error, cluster_pm_dec_error], axis=1)\n",
    "\n",
    "    # **Perform Extreme Deconvolution multiple times & select the best result**\n",
    "    best_result = None\n",
    "    best_log_likelihood = -np.inf\n",
    "\n",
    "    for run in range(5):\n",
    "        # Initialize two Gaussians (background & signal)\n",
    "        init_mean = np.array([\n",
    "            [ra_med, dec_med, (pm_ra_min + pm_ra_max) / 2, (pm_dec_min + pm_dec_max) / 2],  # Signal\n",
    "            [ra_med, dec_med, 0, 0]  # Background (assuming mean ~0 in PM)\n",
    "        ])\n",
    "        init_cov = np.array([np.identity(4) * 0.1, np.identity(4) * 10])  # Signal tight, background broad\n",
    "        init_weights = np.array([0.5, 0.5])  # Equal initial weighting\n",
    "\n",
    "        # Perform Extreme Deconvolution\n",
    "        try:\n",
    "            log_likelihood = extreme_deconvolution(cluster_data_set, cluster_data_set_error, init_mean, init_cov, init_weights, maxiter=1000)\n",
    "            post_XD_means, post_XD_cov = init_mean.copy(), init_cov.copy()\n",
    "\n",
    "            # Choose the best result based on log-likelihood\n",
    "            if log_likelihood > best_log_likelihood:\n",
    "                best_log_likelihood = log_likelihood\n",
    "                best_result = {\n",
    "                    'cluster_id': cluster_id,\n",
    "                    'mean': post_XD_means,\n",
    "                    'covariance': post_XD_cov,\n",
    "                    'log_likelihood': log_likelihood\n",
    "                }\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"XD failed for cluster {cluster_id} on run {run}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # **Select the \"signal\" Gaussian (smallest covariance determinant)**\n",
    "        if best_result:\n",
    "            cov_dets = [det(cov) for cov in best_result['covariance']]\n",
    "            signal_idx = np.argmin(cov_dets)  # The component with the smallest determinant is the \"signal\"\n",
    "\n",
    "            final_results.append({\n",
    "                'cluster_id': best_result['cluster_id'],\n",
    "                'mean': best_result['mean'][signal_idx],  # Only store the signal component\n",
    "                'covariance': best_result['covariance'][signal_idx],\n",
    "                'log_likelihood': best_result['log_likelihood']\n",
    "            })\n",
    "\n",
    "# Save only the final results\n",
    "save_file_path = 'data/glob_clust_XD.pkl'\n",
    "with open(save_file_path, 'wb') as f:\n",
    "    pickle.dump(final_results, f)\n",
    "\n",
    "print(f\"Results saved successfully to {save_file_path}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GA_CW_Venv)",
   "language": "python",
   "name": "ga_cw_venv_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
